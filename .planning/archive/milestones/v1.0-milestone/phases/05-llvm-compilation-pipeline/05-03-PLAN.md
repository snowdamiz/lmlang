---
phase: 05-llvm-compilation-pipeline
plan: 03
type: execute
wave: 3
depends_on: [05-02]
files_modified:
  - crates/lmlang-codegen/src/compiler.rs
  - crates/lmlang-codegen/src/lib.rs
  - crates/lmlang-server/src/handlers/compile.rs
  - crates/lmlang-server/src/handlers/mod.rs
  - crates/lmlang-server/src/router.rs
  - crates/lmlang-server/src/service.rs
  - crates/lmlang-server/src/schema/compile.rs
  - crates/lmlang-server/src/schema/mod.rs
  - crates/lmlang-server/Cargo.toml
  - crates/lmlang-cli/Cargo.toml
  - crates/lmlang-cli/src/main.rs
autonomous: true
requirements: [EXEC-03, EXEC-04]

must_haves:
  truths:
    - "A ProgramGraph compiles to a native binary through the full pipeline: type check -> codegen -> optimize -> link"
    - "Function-scoped Context pattern: Context created, used, and dropped within single compile() call with no LLVM types escaping"
    - "POST /programs/{id}/compile triggers compilation and returns binary path, target triple, binary size, compilation time"
    - "Compilation runs type checker before codegen -- invalid graphs are rejected"
    - "Cross-compilation works via target triple parameter"
    - "Optimization levels O0-O3 work via Module::run_passes"
    - "'lmlang compile' CLI command triggers the same pipeline as the HTTP endpoint and produces a native binary"
  artifacts:
    - path: "crates/lmlang-codegen/src/compiler.rs"
      provides: "Top-level compile() function orchestrating the full pipeline"
      min_lines: 100
    - path: "crates/lmlang-server/src/handlers/compile.rs"
      provides: "HTTP handler for POST /programs/{id}/compile"
    - path: "crates/lmlang-server/src/schema/compile.rs"
      provides: "CompileRequest and CompileResponse API schema types"
    - path: "crates/lmlang-cli/src/main.rs"
      provides: "CLI binary entry point with 'lmlang compile' clap subcommand"
  key_links:
    - from: "crates/lmlang-codegen/src/compiler.rs"
      to: "crates/lmlang-codegen/src/codegen.rs"
      via: "compile_function called for each function in graph"
      pattern: "compile_function"
    - from: "crates/lmlang-codegen/src/compiler.rs"
      to: "crates/lmlang-codegen/src/linker.rs"
      via: "link_executable called after object file emission"
      pattern: "link_executable"
    - from: "crates/lmlang-server/src/handlers/compile.rs"
      to: "crates/lmlang-codegen"
      via: "calls lmlang_codegen::compile()"
      pattern: "lmlang_codegen::compile"
    - from: "crates/lmlang-server/src/router.rs"
      to: "crates/lmlang-server/src/handlers/compile.rs"
      via: "POST /programs/{id}/compile route"
      pattern: "compile"
    - from: "crates/lmlang-cli/src/main.rs"
      to: "lmlang_codegen::compile"
      via: "CLI compile subcommand calls lmlang_codegen::compile()"
      pattern: "lmlang_codegen::compile"
---

<objective>
Implement the top-level compiler that orchestrates the full pipeline (type check -> Context creation -> function compilation -> optimization -> object emission -> linking), and integrate it into both the HTTP server and CLI.

Purpose: Connects all the pieces from Plans 01 and 02 into a working end-to-end compilation pipeline, and exposes it through the user-decided HTTP and CLI entry points.
Output: Working `compile()` function, HTTP endpoint, and CLI command that produce native binaries from graph programs.
</objective>

<execution_context>
@/Users/sn0w/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sn0w/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-llvm-compilation-pipeline/05-RESEARCH.md
@.planning/phases/05-llvm-compilation-pipeline/05-CONTEXT.md
@.planning/phases/05-llvm-compilation-pipeline/05-01-SUMMARY.md
@.planning/phases/05-llvm-compilation-pipeline/05-02-SUMMARY.md
@crates/lmlang-codegen/src/lib.rs
@crates/lmlang-codegen/src/codegen.rs
@crates/lmlang-codegen/src/types.rs
@crates/lmlang-codegen/src/runtime.rs
@crates/lmlang-codegen/src/linker.rs
@crates/lmlang-server/src/router.rs
@crates/lmlang-server/src/service.rs
@crates/lmlang-server/src/handlers/mod.rs
@crates/lmlang-server/src/schema/mod.rs
@crates/lmlang-server/Cargo.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Top-level compiler with function-scoped Context pattern</name>
  <files>
    crates/lmlang-codegen/src/compiler.rs
    crates/lmlang-codegen/src/lib.rs
  </files>
  <action>
Implement compiler.rs with the top-level `compile()` function and the main wrapper generator.

**compile() function (EXEC-04: function-scoped Context):**
```rust
pub fn compile(graph: &ProgramGraph, options: &CompileOptions) -> Result<CompileResult, CodegenError>
```

Implementation (all LLVM work happens within this function scope):
1. Start timer (`std::time::Instant::now()`)
2. Run type checker: `lmlang_check::typecheck::validate_graph(graph)` -- if errors, return `CodegenError::TypeCheckFailed`
3. Create output directory: `std::fs::create_dir_all(&options.output_dir)?`
4. Initialize LLVM targets:
   - If `options.target_triple` is Some: initialize all targets (`Target::initialize_all(&InitializationConfig::default())`)
   - If None: `Target::initialize_native(&InitializationConfig::default())?`
5. **Create fresh Context** -- this is the EXEC-04 boundary:
   ```rust
   let context = Context::create();
   ```
6. Create module: `context.create_module("lmlang_program")`
7. Create builder: `context.create_builder()`
8. Set target triple on module:
   - If target specified: `TargetTriple::create(&triple)`
   - If not: `TargetMachine::get_default_triple()`
9. Declare runtime functions: `runtime::declare_runtime_functions(&context, &module)`
10. Emit runtime error function body: `runtime::emit_runtime_error_fn(&context, &module, &builder)`
11. Compile each function:
    ```rust
    for (func_id, func_def) in graph.functions() {
        codegen::compile_function(&context, &module, &builder, graph, *func_id, func_def)?;
    }
    ```
12. Generate main wrapper: `generate_main_wrapper(&context, &module, &builder, graph, &options)?`
13. Verify module: `module.verify().map_err(|e| CodegenError::LlvmError(e.to_string()))?`
14. Create TargetMachine:
    ```rust
    let target = Target::from_triple(&triple)?;
    let target_machine = target.create_target_machine(
        &triple,
        "generic",        // CPU
        "",               // features
        opt_to_llvm(options.opt_level),  // OptimizationLevel
        RelocMode::Default,
        CodeModel::Default,
    )?;
    ```
15. Run optimization passes (New Pass Manager):
    ```rust
    let pass_options = PassBuilderOptions::create();
    let pass_str = match options.opt_level {
        OptLevel::O0 => "default<O0>",
        OptLevel::O1 => "default<O1>",
        OptLevel::O2 => "default<O2>",
        OptLevel::O3 => "default<O3>",
    };
    module.run_passes(pass_str, &target_machine, pass_options)?;
    ```
16. Write object file to temp directory:
    ```rust
    let temp_dir = tempfile::tempdir()?;
    let obj_path = temp_dir.path().join("output.o");
    target_machine.write_to_file(&module, FileType::Object, &obj_path)?;
    ```
17. Determine output binary name (from entry function name or "program")
18. Link: `linker::link_executable(&obj_path, &output_path, options.debug_symbols)?`
19. Compute binary size: `std::fs::metadata(&output_path)?.len()`
20. Compute compilation time
21. Return CompileResult with binary_path, target_triple (as string), binary_size, compilation_time_ms
22. **Context drops here** -- all LLVM IR freed, no types escape

**generate_main_wrapper:**
- Determine entry function: use `options.entry_function` if specified, otherwise find the first function with name "main", or the first public function, or the first function
- Create `main` function with signature `i32 @main()`
- Call the entry function with no arguments (if it takes parameters, error out -- entry function must be zero-arg)
- If entry function returns integer type, use as exit code; otherwise return 0
- `builder.build_return(Some(&i32_const_0))`

**Helper function `opt_to_llvm`:**
Maps `OptLevel` to inkwell's `OptimizationLevel` enum.

Register `pub mod compiler;` in lib.rs. Also re-export `compile` from lib.rs for clean public API:
```rust
pub use compiler::compile;
```

Add a helper function `compile_to_string` for testing/debugging that returns LLVM IR as a string instead of producing a binary:
```rust
pub fn compile_to_ir(graph: &ProgramGraph, options: &CompileOptions) -> Result<String, CodegenError>
```
Same as compile() but calls `module.print_to_string()` instead of writing object file + linking. Useful for integration tests.
  </action>
  <verify>
`cargo check -p lmlang-codegen` compiles. The `compile` function exists and handles the full pipeline. The Context is created and dropped within the function scope (EXEC-04). Module verification is called.
  </verify>
  <done>
Top-level compile() orchestrates type check -> Context -> codegen -> optimize -> object emit -> link. Context is function-scoped per EXEC-04. Main wrapper generated. Both compile() and compile_to_ir() available. Cross-compilation supported via target triple.
  </done>
</task>

<task type="auto">
  <name>Task 2: HTTP compile endpoint and server integration</name>
  <files>
    crates/lmlang-server/src/handlers/compile.rs
    crates/lmlang-server/src/handlers/mod.rs
    crates/lmlang-server/src/router.rs
    crates/lmlang-server/src/service.rs
    crates/lmlang-server/src/schema/compile.rs
    crates/lmlang-server/src/schema/mod.rs
    crates/lmlang-server/Cargo.toml
  </files>
  <action>
Integrate the compilation pipeline into the HTTP server.

**Cargo.toml:** Add `lmlang-codegen = { path = "../lmlang-codegen" }` to lmlang-server dependencies.

**schema/compile.rs:**
```rust
#[derive(Debug, Deserialize)]
pub struct CompileRequest {
    /// Optimization level (default: "O0")
    #[serde(default = "default_opt_level")]
    pub opt_level: String,  // "O0", "O1", "O2", "O3"
    /// LLVM target triple for cross-compilation (default: host)
    pub target_triple: Option<String>,
    /// Include debug symbols (default: false)
    #[serde(default)]
    pub debug_symbols: bool,
    /// Entry function name (default: auto-detect)
    pub entry_function: Option<String>,
    /// Output directory (default: "./build/")
    pub output_dir: Option<String>,
}

#[derive(Debug, Serialize)]
pub struct CompileResponse {
    pub binary_path: String,
    pub target_triple: String,
    pub binary_size: u64,
    pub compilation_time_ms: u64,
}
```

**schema/mod.rs:** Add `pub mod compile;`

**service.rs:** Add a `compile` method to ProgramService:
```rust
pub fn compile(&self, request: &CompileRequest) -> Result<CompileResponse, ApiError> {
    let options = CompileOptions {
        output_dir: request.output_dir.as_ref()
            .map(PathBuf::from)
            .unwrap_or_else(|| PathBuf::from("./build")),
        opt_level: parse_opt_level(&request.opt_level)?,
        target_triple: request.target_triple.clone(),
        debug_symbols: request.debug_symbols,
        entry_function: request.entry_function.clone(),
    };

    let result = lmlang_codegen::compile(&self.graph, &options)
        .map_err(|e| ApiError::InternalError(e.to_string()))?;

    Ok(CompileResponse {
        binary_path: result.binary_path.to_string_lossy().to_string(),
        target_triple: result.target_triple,
        binary_size: result.binary_size,
        compilation_time_ms: result.compilation_time_ms,
    })
}
```

Add From<CodegenError> to ApiError if needed, or map explicitly. TypeCheckFailed should map to 422 (ValidationFailed), other errors to 500.

**handlers/compile.rs:** Thin handler following established pattern (matches simulate.rs):
```rust
pub async fn compile_program(
    State(state): State<AppState>,
    Path(program_id): Path<i64>,
    Json(request): Json<CompileRequest>,
) -> Result<Json<CompileResponse>, ApiError> {
    let service = state.service.lock().unwrap();

    let active_id = service.program_id();
    if active_id.0 != program_id {
        return Err(ApiError::BadRequest(format!(
            "program {} is not the active program (active: {})",
            program_id, active_id.0
        )));
    }

    let result = service.compile(&request)?;
    Ok(Json(result))
}
```

**handlers/mod.rs:** Add `pub mod compile;`

**router.rs:** Add route:
```rust
.route(
    "/programs/{id}/compile",
    post(handlers::compile::compile_program),
)
```

Per user decision: compile response includes binary_path, target_triple, binary_size, compilation_time. Compilation always runs type checker before codegen.
  </action>
  <verify>
`cargo check -p lmlang-server` compiles. The POST /programs/{id}/compile route exists in the router. The compile handler follows the thin handler pattern.
  </verify>
  <done>
POST /programs/{id}/compile endpoint working. CompileRequest accepts opt_level, target_triple, debug_symbols, entry_function, output_dir. CompileResponse includes binary_path, target_triple, binary_size, compilation_time_ms. Type checker runs before codegen. Server compiles without errors.
  </done>
</task>

<task type="auto">
  <name>Task 3: CLI binary entry point with 'lmlang compile' subcommand</name>
  <files>
    crates/lmlang-cli/Cargo.toml
    crates/lmlang-cli/src/main.rs
  </files>
  <action>
Create the `lmlang-cli` crate as a new workspace member providing the `lmlang` binary with a `compile` subcommand. Per locked user decision: "Both HTTP endpoint (POST /programs/{id}/compile) and CLI command (lmlang compile) â€” same underlying pipeline, two entry points."

**Cargo.toml:**
```toml
[package]
name = "lmlang-cli"
version = "0.1.0"
edition = "2021"

[[bin]]
name = "lmlang"
path = "src/main.rs"

[dependencies]
lmlang-codegen = { path = "../lmlang-codegen" }
lmlang-core = { path = "../lmlang-core" }
lmlang-store = { path = "../lmlang-store" }
clap = { version = "4", features = ["derive"] }
serde_json = "1"
```

**src/main.rs:**

```rust
use clap::{Parser, Subcommand};
use std::path::PathBuf;

#[derive(Parser)]
#[command(name = "lmlang", about = "LM Language compiler and tools")]
struct Cli {
    #[command(subcommand)]
    command: Commands,
}

#[derive(Subcommand)]
enum Commands {
    /// Compile a program to a native binary
    Compile {
        /// Path to the program database file
        #[arg(short, long)]
        db: String,

        /// Program ID to compile
        #[arg(short, long)]
        program: i64,

        /// Optimization level: O0, O1, O2, O3
        #[arg(short, long, default_value = "O0")]
        opt_level: String,

        /// LLVM target triple for cross-compilation (default: host)
        #[arg(short, long)]
        target: Option<String>,

        /// Include debug symbols
        #[arg(long)]
        debug_symbols: bool,

        /// Entry function name (default: auto-detect)
        #[arg(long)]
        entry: Option<String>,

        /// Output directory (default: ./build/)
        #[arg(short = 'O', long, default_value = "./build")]
        output_dir: PathBuf,
    },
}
```

Implementation of the compile subcommand:
1. Open the program database via `lmlang_store` to load the `ProgramGraph`
2. Parse `opt_level` string to `lmlang_codegen::OptLevel` enum
3. Build `CompileOptions` from CLI args (same fields as the HTTP CompileRequest)
4. Call `lmlang_codegen::compile(&graph, &options)` -- the same function the HTTP handler uses
5. On success: print JSON `CompileResult` to stdout (binary_path, target_triple, binary_size, compilation_time_ms) for machine-readable output
6. On error: print error message to stderr and exit with non-zero code
7. Exit codes: 0 = success, 1 = compilation error, 2 = type check failure, 3 = I/O error

The CLI binary is named `lmlang` (via `[[bin]]`), so the command is `lmlang compile --db path --program 1`.
  </action>
  <verify>
`cargo check -p lmlang-cli` compiles. `cargo build -p lmlang-cli` produces a `lmlang` binary. Running `lmlang --help` shows the `compile` subcommand. Running `lmlang compile --help` shows all expected flags (db, program, opt-level, target, debug-symbols, entry, output-dir).
  </verify>
  <done>
`lmlang compile` CLI command exists, accepts all compilation options as CLI flags, calls the same `lmlang_codegen::compile()` function as the HTTP endpoint, and outputs CompileResult as JSON on success. Both entry points (HTTP and CLI) share the identical compilation pipeline per locked user decision.
  </done>
</task>

</tasks>

<verification>
- `cargo check -p lmlang-codegen` and `cargo check -p lmlang-server` and `cargo check -p lmlang-cli` all compile
- compile() function creates and drops Context within its scope (EXEC-04)
- compile() runs type checker before codegen
- compile() supports cross-compilation via target triple
- compile() supports O0/O1/O2/O3 optimization levels via Module::run_passes
- POST /programs/{id}/compile route registered in router
- CompileResponse includes all user-decided fields
- `lmlang compile --help` shows all expected flags
- Both HTTP and CLI entry points call the same lmlang_codegen::compile() function
</verification>

<success_criteria>
- Full compilation pipeline works: ProgramGraph -> type check -> LLVM IR -> optimize -> object file -> linked executable
- Context is function-scoped: created and dropped within compile() (EXEC-04)
- Cross-compilation supported via target triple (EXEC-03)
- HTTP endpoint exposed at POST /programs/{id}/compile with correct request/response schema
- CLI command `lmlang compile` exposed with equivalent options per locked user decision
- Type checker runs before codegen per user decision
- All optimization levels (O0-O3) work
- `cargo check` passes for codegen, server, and cli crates
</success_criteria>

<output>
After completion, create `.planning/phases/05-llvm-compilation-pipeline/05-03-SUMMARY.md`
</output>
