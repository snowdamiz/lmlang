---
phase: 07-multi-agent-concurrency
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - crates/lmlang-server/Cargo.toml
  - crates/lmlang-server/src/lib.rs
  - crates/lmlang-server/src/error.rs
  - crates/lmlang-server/src/state.rs
  - crates/lmlang-server/src/concurrency/mod.rs
  - crates/lmlang-server/src/concurrency/agent.rs
  - crates/lmlang-server/src/concurrency/lock_manager.rs
  - crates/lmlang-server/src/concurrency/conflict.rs
  - crates/lmlang-server/src/schema/mod.rs
  - crates/lmlang-server/src/schema/agents.rs
  - crates/lmlang-server/src/schema/locks.rs
autonomous: true
requirements:
  - MAGENT-01
  - MAGENT-02

must_haves:
  truths:
    - "LockManager tracks per-function read-write locks with TTL auto-expiry"
    - "AgentRegistry manages session-based agent identification via UUID"
    - "Batch lock acquisition sorts by FunctionId and uses all-or-nothing semantics"
    - "AppState uses tokio::sync::RwLock instead of std::sync::Mutex"
    - "Conflict detection compares per-function blake3 hashes and generates structured diffs"
    - "Global write lock serializes module structure changes"
  artifacts:
    - path: "crates/lmlang-server/src/concurrency/lock_manager.rs"
      provides: "FunctionLockState, LockManager with acquire/release/batch/sweep/status"
      min_lines: 150
    - path: "crates/lmlang-server/src/concurrency/agent.rs"
      provides: "AgentId, AgentSession, AgentRegistry"
      min_lines: 50
    - path: "crates/lmlang-server/src/concurrency/conflict.rs"
      provides: "ConflictDetail, FunctionDiff, conflict detection and diff generation"
      min_lines: 60
    - path: "crates/lmlang-server/src/state.rs"
      provides: "AppState with RwLock<ProgramService> + LockManager + AgentRegistry"
      contains: "tokio::sync::RwLock"
    - path: "crates/lmlang-server/src/schema/locks.rs"
      provides: "Lock request/response types for HTTP API"
      min_lines: 40
    - path: "crates/lmlang-server/src/schema/agents.rs"
      provides: "Agent registration request/response types"
      min_lines: 20
  key_links:
    - from: "crates/lmlang-server/src/concurrency/lock_manager.rs"
      to: "lmlang-core::id::FunctionId"
      via: "DashMap<FunctionId, FunctionLockState>"
      pattern: "DashMap.*FunctionId"
    - from: "crates/lmlang-server/src/concurrency/conflict.rs"
      to: "lmlang-storage::hash::hash_function"
      via: "blake3 per-function hashing for conflict detection"
      pattern: "hash_function"
    - from: "crates/lmlang-server/src/state.rs"
      to: "crates/lmlang-server/src/concurrency/lock_manager.rs"
      via: "AppState holds Arc<LockManager>"
      pattern: "Arc<LockManager>"
---

<objective>
Build the core concurrency infrastructure for multi-agent support: LockManager with per-function read-write locks, AgentRegistry for session management, conflict detection via blake3 hashing, and refactor AppState from Mutex to tokio::sync::RwLock.

Purpose: This provides the foundational types and state management that all subsequent concurrency work (handlers, verification, tests) depends on.
Output: New `concurrency/` module with LockManager, AgentRegistry, and conflict detection; refactored AppState; new schema types for lock/agent API.
</objective>

<execution_context>
@/Users/sn0w/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sn0w/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-multi-agent-concurrency/07-CONTEXT.md
@.planning/phases/07-multi-agent-concurrency/07-RESEARCH.md
@crates/lmlang-server/src/state.rs
@crates/lmlang-server/src/error.rs
@crates/lmlang-server/src/lib.rs
@crates/lmlang-server/Cargo.toml
@crates/lmlang-server/src/schema/mod.rs
@crates/lmlang-core/src/id.rs
@crates/lmlang-storage/src/hash.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create concurrency module with LockManager, AgentRegistry, and conflict detection</name>
  <files>
    crates/lmlang-server/Cargo.toml
    crates/lmlang-server/src/lib.rs
    crates/lmlang-server/src/concurrency/mod.rs
    crates/lmlang-server/src/concurrency/agent.rs
    crates/lmlang-server/src/concurrency/lock_manager.rs
    crates/lmlang-server/src/concurrency/conflict.rs
    crates/lmlang-server/src/schema/mod.rs
    crates/lmlang-server/src/schema/agents.rs
    crates/lmlang-server/src/schema/locks.rs
  </files>
  <action>
    **Add dependency:** Add `dashmap = "6"` to lmlang-server/Cargo.toml.

    **Create `concurrency/agent.rs`:**
    - `AgentId(pub Uuid)` newtype with Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize.
    - `AgentSession` struct: `id: AgentId`, `name: Option<String>`, `registered_at: Instant`, `last_active: Instant`.
    - `AgentRegistry` struct backed by `DashMap<AgentId, AgentSession>`.
    - Methods: `register(name: Option<String>) -> AgentId` (generates UUID v4), `deregister(id: &AgentId) -> bool`, `get(id: &AgentId) -> Option<AgentSession>`, `list() -> Vec<AgentSession>`, `touch(id: &AgentId)` (updates last_active).
    - Session inactivity timeout default: 1 hour. Method `sweep_inactive(timeout: Duration)` removes sessions inactive longer than timeout.

    **Create `concurrency/lock_manager.rs`:**
    - `LockMode` enum: `Read`, `Write` with Serialize, Deserialize, Debug, Clone, Copy.
    - `LockHolderInfo` struct: `agent_id: AgentId`, `agent_name: Option<String>`, `description: Option<String>`, `acquired_at: Instant`, `expires_at: Instant`.
    - `FunctionLockState` enum: `Unlocked`, `ReadLocked { readers: HashMap<AgentId, Instant>, expires_at: Instant }`, `WriteLocked { holder: LockHolderInfo }`.
    - `LockGrant` struct (Serialize): `function_id: FunctionId`, `mode: LockMode`, `expires_at: String` (ISO 8601 formatted).
    - `LockDenial` struct (Serialize): `function_id: FunctionId`, `holder_agent_id: AgentId`, `holder_name: Option<String>`, `holder_description: Option<String>`, `queue_position: usize`.
    - `LockError` enum (thiserror): `AlreadyHeldBy(LockDenial)`, `NotHeld { function_id: FunctionId, agent_id: AgentId }`, `FunctionNotFound(FunctionId)`, `BatchPartialFailure { acquired: Vec<FunctionId>, failed: LockDenial }`.
    - `LockStatusEntry` struct (Serialize): `function_id: FunctionId`, `state: String` (read/write/unlocked), `holders: Vec<AgentId>`, `holder_description: Option<String>`, `expires_at: Option<String>`.
    - `LockManager` struct: `function_locks: DashMap<FunctionId, FunctionLockState>`, `global_write_lock: tokio::sync::RwLock<()>`, `default_ttl: Duration`.
    - Constructor: `new(default_ttl: Duration) -> Self` with 30-minute default TTL.
    - `try_acquire_read(&self, agent_id: &AgentId, func_id: FunctionId) -> Result<LockGrant, LockError>`: If unlocked or ReadLocked, add reader. If WriteLocked by same agent, allow (upgrade). If WriteLocked by another, deny with LockDenial.
    - `try_acquire_write(&self, agent_id: &AgentId, func_id: FunctionId, description: Option<String>) -> Result<LockGrant, LockError>`: If unlocked, acquire. If ReadLocked only by this agent, upgrade to write. If held by another, deny.
    - `batch_acquire_write(&self, agent_id: &AgentId, function_ids: &[FunctionId], description: Option<String>) -> Result<Vec<LockGrant>, LockError>`: Sort IDs by FunctionId.0, dedup, acquire all-or-nothing (release on failure per CONTEXT.md).
    - `release(&self, agent_id: &AgentId, func_id: FunctionId) -> Result<(), LockError>`: Remove agent from readers or release write lock. Remove entry from DashMap if unlocked.
    - `release_all(&self, agent_id: &AgentId) -> Vec<FunctionId>`: Release all locks held by agent. Returns list of released function IDs.
    - `verify_write_locks(&self, agent_id: &AgentId, function_ids: &[FunctionId]) -> Result<(), LockError>`: Verify agent holds write locks for all specified functions.
    - `status(&self) -> Vec<LockStatusEntry>`: Returns current lock state for all locked functions.
    - `sweep_expired_locks(&self) -> Vec<FunctionId>`: Remove expired locks, return list of released function IDs.
    - `start_expiry_sweep(self: &Arc<Self>, interval: Duration)`: Spawns tokio background task that calls `sweep_expired_locks` periodically.

    **Create `concurrency/conflict.rs`:**
    - `ConflictDetail` struct (Serialize): `function_id: FunctionId`, `expected_hash: String`, `current_hash: String`, `changes: FunctionDiff`.
    - `FunctionDiff` struct (Serialize): `added_nodes: Vec<NodeId>`, `removed_nodes: Vec<NodeId>`, `modified_nodes: Vec<NodeId>`, `added_edges: Vec<EdgeId>`, `removed_edges: Vec<EdgeId>`.
    - `check_hashes(graph: &ProgramGraph, expected_hashes: &HashMap<FunctionId, String>) -> Result<(), Vec<ConflictDetail>>`: For each entry, compute current hash via `lmlang_storage::hash::hash_function()`, compare hex strings. On mismatch, call `build_function_diff` and collect conflicts.
    - `build_function_diff(graph: &ProgramGraph, func_id: FunctionId) -> FunctionDiff`: Use `graph.function_nodes(func_id)` to enumerate current nodes and edges within the function. Since we lack the "before" snapshot, the diff shows current state nodes/edges. The conflict detail with hash mismatch plus the current function structure is sufficient for agents to understand what changed and re-plan their edits.

    **Create `concurrency/mod.rs`:** Re-export agent, lock_manager, conflict submodules.

    **Create `schema/agents.rs`:**
    - `RegisterAgentRequest` (Deserialize): `name: Option<String>`.
    - `RegisterAgentResponse` (Serialize): `agent_id: Uuid`, `name: Option<String>`, `registered_at: String`.
    - `AgentView` (Serialize): `agent_id: Uuid`, `name: Option<String>`.
    - `ListAgentsResponse` (Serialize): `agents: Vec<AgentView>`.

    **Create `schema/locks.rs`:**
    - `AcquireLockRequest` (Deserialize): `function_ids: Vec<u32>`, `mode: String` ("read" or "write"), `description: Option<String>`.
    - `AcquireLockResponse` (Serialize): `grants: Vec<LockGrantView>`.
    - `LockGrantView` (Serialize): `function_id: u32`, `mode: String`, `expires_at: String`.
    - `ReleaseLockRequest` (Deserialize): `function_ids: Vec<u32>`.
    - `ReleaseLockResponse` (Serialize): `released: Vec<u32>`.
    - `LockStatusResponse` (Serialize): `locks: Vec<LockStatusView>`.
    - `LockStatusView` (Serialize): `function_id: u32`, `state: String`, `holders: Vec<String>`, `holder_description: Option<String>`, `expires_at: Option<String>`.

    **Update `schema/mod.rs`:** Add `pub mod agents;` and `pub mod locks;`.
    **Update `lib.rs`:** Add `pub mod concurrency;`.
  </action>
  <verify>
    `cargo check --package lmlang-server` compiles with no errors. All new types are accessible from the crate root.
  </verify>
  <done>
    New `concurrency/` module exists with LockManager (per-function RW locks, batch acquire, TTL sweep), AgentRegistry (session management), and conflict detection (hash-based). Schema types for lock and agent APIs exist. DashMap dependency added.
  </done>
</task>

<task type="auto">
  <name>Task 2: Refactor AppState and error types for concurrent access</name>
  <files>
    crates/lmlang-server/src/state.rs
    crates/lmlang-server/src/error.rs
    crates/lmlang-server/src/handlers/mutations.rs
    crates/lmlang-server/src/handlers/queries.rs
    crates/lmlang-server/src/handlers/programs.rs
    crates/lmlang-server/src/handlers/verify.rs
    crates/lmlang-server/src/handlers/simulate.rs
    crates/lmlang-server/src/handlers/history.rs
    crates/lmlang-server/src/handlers/compile.rs
    crates/lmlang-server/src/handlers/contracts.rs
  </files>
  <action>
    **Refactor `state.rs`:**
    Replace `Arc<Mutex<ProgramService>>` with `Arc<tokio::sync::RwLock<ProgramService>>`. Add `lock_manager: Arc<LockManager>` and `agent_registry: Arc<AgentRegistry>` fields.

    ```rust
    pub struct AppState {
        pub service: Arc<tokio::sync::RwLock<ProgramService>>,
        pub lock_manager: Arc<LockManager>,
        pub agent_registry: Arc<AgentRegistry>,
    }
    ```

    Update `AppState::new()` and `AppState::in_memory()`:
    - Create LockManager with 30-minute default TTL.
    - Create AgentRegistry.
    - Wrap ProgramService in `tokio::sync::RwLock`.
    - Start the expiry sweep task (every 60 seconds) on the LockManager.

    **Extend `error.rs`:**
    Add new ApiError variants:
    - `LockDenied(LockDenial)` -> 423 (Locked) status code. Response body includes holder info and queue position.
    - `LockRequired(String)` -> 428 (Precondition Required). Agent must acquire lock first.
    - `AgentRequired(String)` -> 400. Missing X-Agent-Id header.
    - `TooManyRetries(String)` -> 429.
    Add `From<LockError>` impl mapping LockError variants to appropriate ApiError variants.

    **Refactor ALL existing handlers** to use `tokio::sync::RwLock` instead of `Mutex`:
    - Read-only handlers (queries, simulate, verify, history listing, dirty_status, list_programs, program_overview, get_node, get_function, neighborhood, search, list_checkpoints, list_history, diff_versions): Change `state.service.lock().unwrap()` to `state.service.read().await`.
    - Write handlers (propose_edit, undo, redo, create_checkpoint, restore_checkpoint, create_program, load_program, delete_program, compile_program, property_test): Change `state.service.lock().unwrap()` to `state.service.write().await`.
    - Note: compile_incremental needs `&mut self` so uses write lock. Regular compile only needs `&self` so uses read lock.

    **Important:** At this point, existing handlers do NOT require agent identity or function locks -- they work as before but with RwLock instead of Mutex. Agent-aware locking is added in Plan 02 for mutations. This ensures backward compatibility: single-agent usage continues to work without changes.

    **Helper function `extract_agent_id`:** Add to `concurrency/mod.rs`:
    ```rust
    pub fn extract_agent_id(headers: &axum::http::HeaderMap) -> Result<AgentId, ApiError> {
        headers.get("X-Agent-Id")
            .and_then(|v| v.to_str().ok())
            .and_then(|s| Uuid::parse_str(s).ok())
            .map(AgentId)
            .ok_or_else(|| ApiError::AgentRequired("X-Agent-Id header required".to_string()))
    }
    ```
    This will be used by lock/agent handlers in Plan 02.
  </action>
  <verify>
    `cargo check --package lmlang-server` compiles. `cargo test --package lmlang-server` passes all existing tests. The RwLock migration must not break any existing functionality.
  </verify>
  <done>
    AppState uses tokio::sync::RwLock with LockManager and AgentRegistry. All existing handlers use read/write await instead of mutex lock. Error types extended with lock-related variants. extract_agent_id helper ready for use. All existing tests pass.
  </done>
</task>

</tasks>

<verification>
1. `cargo check --package lmlang-server` succeeds with zero errors
2. `cargo test --package lmlang-server` passes all existing tests (RwLock migration is backward-compatible)
3. LockManager can be constructed and its methods compile
4. AgentRegistry can register/deregister agents
5. ConflictDetail types compile and serialize to JSON
</verification>

<success_criteria>
- AppState uses `Arc<tokio::sync::RwLock<ProgramService>>` (not Mutex)
- LockManager tracks per-function locks with DashMap
- AgentRegistry manages sessions with UUID-based agent IDs
- Conflict detection uses existing blake3 hash_function from lmlang-storage
- All existing tests pass with no behavioral changes
- Error types include LockDenied, LockRequired, AgentRequired variants
</success_criteria>

<output>
After completion, create `.planning/phases/07-multi-agent-concurrency/07-01-SUMMARY.md`
</output>
