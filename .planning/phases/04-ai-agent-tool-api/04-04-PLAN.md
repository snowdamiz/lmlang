---
phase: 04-ai-agent-tool-api
plan: 04
type: execute
wave: 4
depends_on: ["04-03"]
files_modified:
  - crates/lmlang-server/tests/integration_test.rs
autonomous: true
requirements:
  - TOOL-01
  - TOOL-02
  - TOOL-03
  - TOOL-04
  - TOOL-05
  - TOOL-06
  - STORE-03

must_haves:
  truths:
    - "An AI agent can create a program, add functions and nodes via HTTP, and retrieve the resulting graph"
    - "Mutations with dry_run=true return validation results without changing the graph"
    - "Batch mutations that fail validation leave the graph unchanged (atomicity)"
    - "Type verification catches type mismatches and returns structured diagnostics with node IDs"
    - "Simulation executes a function and returns correct output values"
    - "Undo reverses the last mutation and the graph matches its pre-mutation state"
    - "Named checkpoints can be created and restored, reverting to the snapshot state"
    - "All error responses include structured error codes and graph location context"
  artifacts:
    - path: "crates/lmlang-server/tests/integration_test.rs"
      provides: "End-to-end integration tests for all 7 requirements"
      min_lines: 200
  key_links:
    - from: "crates/lmlang-server/tests/integration_test.rs"
      to: "crates/lmlang-server/src/router.rs"
      via: "axum::test::TestClient or direct Router testing"
      pattern: "build_router|TestClient"
---

<objective>
Write end-to-end integration tests that exercise the full HTTP API, proving all 7 phase requirements work correctly. Tests use axum's built-in test utilities to send real HTTP requests to the router without starting a network server.

Purpose: Proves the system works end-to-end from HTTP request through handler through ProgramService through graph/storage/checker/interpreter and back. This is the acceptance test for Phase 4.
Output: Integration test suite covering all requirements with at least one test per requirement ID.
</objective>

<execution_context>
@/Users/sn0w/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sn0w/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-ai-agent-tool-api/04-CONTEXT.md
@.planning/phases/04-ai-agent-tool-api/04-RESEARCH.md
@.planning/phases/04-ai-agent-tool-api/04-01-SUMMARY.md
@.planning/phases/04-ai-agent-tool-api/04-02-SUMMARY.md
@.planning/phases/04-ai-agent-tool-api/04-03-SUMMARY.md
@crates/lmlang-server/src/router.rs
@crates/lmlang-server/src/schema/mutations.rs
@crates/lmlang-server/src/schema/queries.rs
@crates/lmlang-server/src/schema/simulate.rs
@crates/lmlang-server/src/schema/history.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: End-to-end integration tests for all phase requirements</name>
  <files>
    crates/lmlang-server/tests/integration_test.rs
  </files>
  <action>
    Use `axum::body::Body` + `tower::ServiceExt` to test the router directly without starting a network server. Each test creates a fresh AppState with an in-memory SQLite database (use ":memory:" as db_path or a temp file).

    Add `axum-test` or use the standard pattern:
    ```rust
    use axum::body::Body;
    use axum::http::{Request, StatusCode};
    use tower::ServiceExt; // for oneshot
    use serde_json::json;
    ```

    Helper function:
    ```rust
    fn test_app() -> Router {
        let state = AppState::new(":memory:").unwrap();
        build_router(state)
    }

    async fn post_json(app: &Router, path: &str, body: serde_json::Value) -> (StatusCode, serde_json::Value) {
        let response = app.clone().oneshot(
            Request::builder()
                .method("POST")
                .uri(path)
                .header("content-type", "application/json")
                .body(Body::from(serde_json::to_vec(&body).unwrap()))
                .unwrap()
        ).await.unwrap();
        let status = response.status();
        let body = axum::body::to_bytes(response.into_body(), usize::MAX).await.unwrap();
        let json: serde_json::Value = serde_json::from_slice(&body).unwrap_or(json!(null));
        (status, json)
    }

    async fn get_json(app: &Router, path: &str) -> (StatusCode, serde_json::Value) {
        let response = app.clone().oneshot(
            Request::builder()
                .uri(path)
                .body(Body::empty())
                .unwrap()
        ).await.unwrap();
        let status = response.status();
        let body = axum::body::to_bytes(response.into_body(), usize::MAX).await.unwrap();
        let json: serde_json::Value = serde_json::from_slice(&body).unwrap_or(json!(null));
        (status, json)
    }
    ```

    Write the following tests (each #[tokio::test]):

    **1. TOOL-01: propose_structured_edit with single mutation**
    - Create a program via POST /programs
    - Load it via POST /programs/{id}/load
    - Add a function via POST /programs/{id}/mutations with Mutation::AddFunction
    - Verify response has valid=true, committed=true, created contains FunctionId
    - Add two Parameter nodes and a BinaryArith(Add) node
    - Add data edges connecting params -> add
    - GET /programs/{id}/overview and verify node_count and function list

    **2. TOOL-01: dry_run previews without committing**
    - Start with a program that has a function
    - POST mutations with dry_run=true to add a node
    - Verify response has valid=true, committed=false
    - GET overview and verify node count has NOT changed

    **3. TOOL-01: batch mutation atomicity (all-or-nothing)**
    - Start with a program that has a function
    - POST batch mutations: first valid (add node), second invalid (add edge to nonexistent node)
    - Verify response has valid=false, committed=false, errors non-empty
    - GET overview and verify graph is unchanged (first mutation was not applied)

    **4. TOOL-02: retrieve_subgraph by node ID**
    - Build a program with nodes and edges
    - GET /programs/{id}/nodes/{node_id}?detail=full
    - Verify response contains node info with op_data, incoming_edges, outgoing_edges

    **5. TOOL-02: retrieve by function boundary**
    - Build a program with two functions
    - GET /programs/{id}/functions/{func_id}?detail=standard
    - Verify response contains only nodes/edges from that function

    **6. TOOL-02: N-hop neighborhood**
    - Build a chain of nodes: A -> B -> C -> D
    - POST /programs/{id}/neighborhood with node_id=A, max_hops=2
    - Verify response contains A, B, C but NOT D

    **7. TOOL-03: verify_and_propagate catches type mismatch**
    - Build a program with an intentional type mismatch (i32 + f64)
    - POST /programs/{id}/verify with scope="full"
    - Verify response has valid=false, errors contains TYPE_MISMATCH code
    - Verify error details include source_node, target_node, expected_type, actual_type

    **8. TOOL-04: simulate function execution**
    - Build a simple add(a: i32, b: i32) -> i32 function
    - POST /programs/{id}/simulate with function_id and inputs [3, 5]
    - Verify response has success=true, result=8
    - Verify with trace_enabled=true, trace entries are present

    **9. TOOL-05: HTTP/JSON endpoints accessible**
    - This is verified by all other tests actually working over HTTP.
    - Additionally: verify Content-Type of responses is application/json.
    - Verify invalid JSON body returns 400 with structured error.

    **10. TOOL-06: structured diagnostics with graph context**
    - Trigger a type error via mutation (add mismatched edge with dry_run=true)
    - Verify error response contains: code, message, details with node IDs and type info
    - Verify error does NOT include fix suggestions (per locked decision)

    **11. STORE-03: undo reverses last mutation**
    - Add a node to a function
    - POST /programs/{id}/undo
    - Verify undo response success=true
    - GET overview and verify node count decreased

    **12. STORE-03: named checkpoint and restore**
    - Build a program with some nodes
    - POST /programs/{id}/checkpoints to create checkpoint "before_changes"
    - Add more nodes (mutate the graph)
    - POST /programs/{id}/checkpoints/before_changes/restore
    - GET overview and verify graph matches the checkpoint state (fewer nodes)

    **13. STORE-03: list history and checkpoints**
    - Perform several mutations
    - Create a checkpoint
    - GET /programs/{id}/history -- verify entries present
    - GET /programs/{id}/checkpoints -- verify checkpoint present with metadata

    Each test should assert HTTP status codes (200 for success, 400/404/422 for errors) and validate response JSON structure.
  </action>
  <verify>`cargo test -p lmlang-server -- --test-threads=1` passes all integration tests (test-threads=1 because tests use in-memory SQLite)</verify>
  <done>13+ integration tests pass covering all 7 requirement IDs: TOOL-01 (single, dry_run, batch atomicity), TOOL-02 (node, function, neighborhood), TOOL-03 (type verification with diagnostics), TOOL-04 (simulation with trace), TOOL-05 (HTTP/JSON format), TOOL-06 (structured error diagnostics), STORE-03 (undo, checkpoint, restore, history)</done>
</task>

</tasks>

<verification>
- `cargo test -p lmlang-server` -- all integration tests pass
- `cargo test --workspace` -- all tests across all crates pass (no regressions)
- Each requirement ID (TOOL-01 through TOOL-06, STORE-03) has at least one dedicated test
- Tests exercise the full stack: HTTP request -> handler -> service -> graph/storage/checker -> response
- Error cases tested: invalid mutations, type mismatches, not-found entities
</verification>

<success_criteria>
- All 13+ integration tests pass
- Full workspace `cargo test` passes with no regressions
- Tests cover the happy path and error paths for each requirement
- Batch atomicity proven: failed batch leaves graph unchanged
- dry_run proven: validation without commit
- Undo proven: mutation reversed, graph restored
- Checkpoint proven: restore returns to snapshot state
- Structured diagnostics proven: error codes + node IDs in response
- Simulation proven: correct output values from interpreter
</success_criteria>

<output>
After completion, create `.planning/phases/04-ai-agent-tool-api/04-04-SUMMARY.md`
</output>
